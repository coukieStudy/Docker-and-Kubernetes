# 11 애플리케이션 배포를 위한 고급 설정

## 11.1 포드의 자원 사용량 제한

- 트래픽이 커질 때 스케일 아웃으로 증설도 중요하지만, Utilization(자원 활용율)을 늘리는 것도 중요하다 -> 컨테이너의 리소스 사용량 제한 필요

### 11.1.1 컨테이너와 포드의 자원 사용량 제한 : Limits

- 자원 할당량 설정 X -> 노드의 물리 자원 모두 사용 가능 -> 고갈 상황 발생 가능

- 예제

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
  	name: resource-limit-pod
  	labels:
  		name: resource-limit-pod
  spec:
  	containers:
  		- name: nginx
  			image: nginx:latest
      resources:
      	limits:
      		memory: "256Mi"
      		cpu: "1000m"
  ```

  - Spec.containers.resources.limits.memory
    - docker run --memory와 같이 256m
  - Spec.containers.resources.limits.cpu
    - docker run --cpus 1 == 1000m(밀리코어) - 최대 1개의 cpu

- `kubectl describe node`

  - 워커 노드의 가용자원 확인 가능

### 11.1.2 컨테이너와 포드의 자원 사용량 제한하기 : Requests

- Requests

  - 컨테이너의 보장되어야 하는 최소 자원

- Overcommit

  - Utilization을 높이기 위한 방법
  - 1GB를 500MB Limit의 컨테이너 2개로 생성 시 Limit은 최대이기 때문에 사용률이 떨어질 수 있음
  - 1GB를 750MB Limit의 컨테이너 2개로 생성해서 다른 컨테이너의 사용량이 적을 때 추가 사용
  - 따라서 Requests로 최소 자원을 정의해서 최소한의 할당량을 보전하고자 함

- Limit and Request

  - 최소한 Request만큼의 사용량은 보장, 유휴 자원이 있으면 최대 Limit까지 사용 가능
  - 노드에 포드를 할당하는 기준은 Limits이 아닌 Requests

- [ ] 어떻게 튜닝해야 할까?

- 예시

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
  	name: resource-limit-with-request-pod
  	labels:
  		name: resource-limit-with-request-pod
  spec:
  	containers:
  		- name: nginx
  			image: nginx:latest
      resources:
      	limits:
      		memory: "256Mi"
      		cpu: "1000m"
        requests:
        	memory: "128Mi"
        	cpu: "500m"
  ```

### 11.1.3 CPU 자원 사용량의 제한 원리

- CPU의 requests는 docker run --cpu-shares 옵션과 동일
  - Cpu-shares의 컨테이너당 할당 비율에 따라 사용
  - 자원이 포화일 때 requests 비율로 나눔
  - 즉 500m 파드가 2개 있다면, 1대 1 비율로 사용하게 됨
- 경합이 발생하는 예시
  1. Limits : 700m, Requests: 500m 인 파드A가 노드에 할당
  2. A 파드는 700m 만큼 cpu 사용
  3. Limits : 700m, Requests: 500m 인 파드B가 노드에 할당
  4. B가 500m 만큼 사용해야 하기 때문에 A 파드에는 스로틀이 발생
- Requests에 할당되지 않은 여유 CPU 자원이 있다면, 남은 자원 역시 Requests의 비율에 맞춰 사용
- 메모리와 다른 점
  - CPU : 압축 가능한 리소스
    - 스로틀을 통해 경합 발생시 사용량 억제 가능
  - 메모리, 스토리지 : 압축 불가능한 리소스
    - 경합 발생시 우선순위 낮은 컨테이너 종료

### 11.1.4 QoS 클래스와 메모리 자원 사용량 제한 원리

- Eviction
  - 메모리는 압축 불가능한 자원으로 경합상황에서 우선순위 낮은 포드 강제 종료
  - 종료된 포드는 다른 노드로 이동
- MemoryPressure 상태
  - 노드의 메모리 부족 시 Conditions에서 MemoryPressure, DiskPressure 등의 True가 됨
  - MemoryPressure 발생 시 우선순위 정하고 Eviction 실행
  - MemoryPressure 상태에서는 포드 할당 X
- 리눅스의 OOM 킬러
  - 갑작스러운 메모리 사용량 증가로 MemoryPressure이 감지 되기 전에 리눅스 기본 설정인 OOM Killer로 컨테이너 자동 종료 가능
  - 도커 데몬의 경우 매우 낮은 OOM 점수(-999)를 할당받아 종료되는 경우가 없게 함
- QoS
  - 포드의 우선순위를 정하기 위해 사용
  - `kubectl describe pod resource-limit-pod | grep QoS`
  - QoS 클래스 종류
    - Guaranteed 클래스
      - Limits과 Requests가 완전히 동일할 때 부여되는 클래스
      - Limits만 설정하면 Requests가 자동으로 설정되기 때문에 Guaranteed로 됨
      - OOM점수가 -998로 거의 종료되는 일 없음
    - BestEffort 클래스
      - Requests와 Limits를 아예 설정하지 않는 포드에 설정
    - burstable 클래스
      - Limits의 값이 Requests보다 큰 포드에 설정
      - Requests보다 많은 자원을 사용하고 있는 포드가 우선순위 낮게 설정
  - Guranteed > Burstable > BestEffort
    - 그러나 Burstable과 BestEffort는 순위가 역전될 수 있다
    - 메모리를 많이 사용할수록 우선순위 낮아짐

### 11.1.5 ResourceQuota와 LimitRange

- 만약 여러 개발팀이 개발 및 테스트를 진행해야 한다면?
  - 쿠버네티스 클러스터를 하나씩 제공할 수 없다
  - 따라서 네임스페이스를 개발팀마다 생성, RBAC를 설정한다
  - 따라서 ResourceQuota와 LimitRange를 이용해 네임스페이스 간의 리소스 할당을 조절

#### ResourceQuota

- > 특정 네임스페이스에서 사용할 수 있는 자원 사용량의 합을 제한할 수 있는 쿠버네티스 오브젝트

- 기능

  - 네임스페이스에서 할당할 수 있는 자원의 총합을 제한
  - 네임스페이스에서 생성할 수 있는 리소스(서비스, 디플로이먼트 등)의 개수를 제한
    - 실수를 방지하기 위해

- `kubectl get quota`

- 예시

  ```yaml
  apiVersion: v1
  kind: ResourceQuota
  metadata:
  	name: resource-quota-example
  	namespace: default
  spec:
  	hard:
  		requests.cpu: "1000m"
  		requests.memory: "500Mi"
  		limits.cpu: "1500m"
  		limits.memory: "1000Mi"
  		count/pods: 3
  		count/services: 5
  ```

  - Count/pods : 포드 리소스 생성 개수 제한
  - QoS 클래스에 따라 제한도 가능

#### LimitRange

- 네임스페이스에서 할당되는 자원의 범위 or 기본값 지정 가능

  - 기본 Requests와 Limits 값 설정
  - CPU, Memory, PVC 스토리지 크기의 최솟값/최댓값 설정

- `kubectl get limitranges`

- 예시

  ```yaml
  apiVersion: v1
  kind: LimitRange
  metadata:
  	name: mem-limit-range
  spec:
  	limits:
  	- default:
  			memory: 256Mi
  			cpu: 200m
  		defaultRequest:
  			memory: 128Mi
  			cpu: 100m
       max:
       	memory: 1Gi
       	cpu: 1000m
       min:
       	memory: 16Mi
       	cpu : 50m
       type: Container
  ```

  - Default : Limits의 디폴트 값
  - defaultRequest: Requests 디폴트 값
  - Type: 자원 할당의 단위
    - pod, pvc 등 입력 가능

- maxLimitRequestRatio

  - ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
    	name: mem-limit-range
    spec:
    	limits:
    	- maxLimitRequestRatio:
    			memory: 1.5
    			cpu: 1
    		type: Container
    ```

  - Limits와 Requests의 비율을 제한 가능

  - 1로 설정하면 Guranteed 클래스 포드만 생성 강제도 가능

### 11.1.6 ResourceQuota, LimitRange의 원리 : Admission Controller

- 그림 11.11
- Admission Controller
  - 인증, 인가 이후에 **사용자 API 요청 적절한지 검증, 필요하면 변형**
  - ServiceAcount, ResourceQuota, LimitRange도 여기에 포함
  - 서비스 메쉬인 Istio도 어드미션 컨트롤러를 통해 사이드가 컨테이너 주입
  - 이 외에도 직접 구현 가능
- Admission Controller의 단계
  - Validating
    - ResourceQuota가 최대 자원 할당 초과시 API 요청 거절
  - Mutating
    - LimitRanger가 기본값으로 API 변형

